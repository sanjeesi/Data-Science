{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjeesi/Notes-Notebooks/blob/master/Data%20Science%20IITM/MLP/Week%205/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05dtk0Wk6zNf"
      },
      "source": [
        "# Classification Functions in Scikit Learn\n",
        "Specific classificaion algorithms\n",
        "- Least square classification\n",
        "- Perceptron\n",
        "- Logistic regression\n",
        "  - With regularization\n",
        "  - multiclass, multilabel and multi-output setting\n",
        "\n",
        "**Cross validation** and **hyper parameter search** for classification works exactly like it works in regression setting.\n",
        "- However there are a couple of CV strategies that are specific to classification\n",
        "\n",
        "\n",
        "Two types of APIs:\n",
        "\n",
        "|Generic|Specific|\n",
        "|---|---|\n",
        "|SGD classifier|Logistic regression|\n",
        "||Perceptron|\n",
        "||Ridge classifier (for LSC)|\n",
        "||K-nearest neighbours (KNNs)|\n",
        "||Support vector machines (SVMs)|\n",
        "||Naive Bayes|\n",
        "|||\n",
        "|uses **gradient descent** for optimization|**Specialized solvers** for opt.|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ridge classifier\n",
        "- classifier variant of the **Ridge** regressor.  \n",
        "\n",
        "Binary classification:\n",
        "- classifier first converts binary targets to {-1, 1} and then treats the problem as a regression task.\n",
        "- sklearn provides different **solvers** for the optimization\n",
        "- predicted class corresponds to the sign of the regressor's prediction  \n",
        "\n",
        "Multiclass classification:\n",
        "- treated as multi-output regression\n",
        "- predicted class corresponds to the output with the highest value.  \n",
        "\n",
        "Use one of the following **solvers**:\n",
        "- `svd` : uses a Singular Value Decomposition of the feature matrix to compute the Ridge coefficients.\n",
        "- `cholesky` : uses `scipy.linalg.solve` function to obtain the closed-form solution\n",
        "- `sparse_cg` : uses the conjugate gradient solver of `scipy.sparse.linalg.cg`.\n",
        "- `lsqr` : uses the dedicated regularized least-squares routine `scipy.sparse.linalg.lsqr` and **it is the fastest**.\n",
        "- `sag`, `saga` : uses Stochastic Average Gradient descent iterative procedure. 'saga' is unbiased and more flexible version of 'sag'\n",
        "- `lbfgs` : uses L-BFGS-B algorithm implemented in `scipy.optimize.minimize`. can be used only when coefficients are forced to be positive.  \n",
        "\n",
        "Choice of **solver** in RidgeClassifier:\n",
        "- For large scale data, use `sparse_cg` solver.\n",
        "- When both n_sampels and n_features are large, use 'sag' or 'saga' solvers.\n",
        "  - Note that fast convergence is only guaranteed on features with approximately the same scale.  \n",
        "\n",
        "### How to make RidgeClassifier select the solver automatically?\n",
        "```ridge_classifier = RidgeClassifier(solver=auto)```  \n",
        "`auto`: chooses the solver automatically based on the type of data.  \n",
        "Default choice for solver is always `auto`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perceptron\n",
        "- It is a simple classification algorithm suitable for **large-scale learning**.\n",
        "- Shares the same underlying implementation with `SGDClassifier`\n",
        "\n",
        "|Both|are Equivalent|\n",
        "|---|---|\n",
        "|`Perceptron()`|```SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\"```|\n",
        "\n",
        "Perceptron uses SGD for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression\n",
        "- a.k.a. logit regression, maximum entropy classifier (maxent) and log-linear classifier.\n",
        "$$\n",
        "arg min_{w,c}\\ regularization\\ penalty\\ +\\ CÃ—cross\\ entropy\\ loss\n",
        "$$  \n",
        "\n",
        "C: **inverse of regularization rate**\n",
        "\n",
        "- This implementation can fit\n",
        "  - binary classification\n",
        "  - one-vs-rest (OVR)\n",
        "  - multinomial logistic regression\n",
        "- Provision for **l1, l2** or **elastic-net regularization**  \n",
        "\n",
        "### How to select **solvers** for Logistic Regression classifier?\n",
        "The choice of the solver depends on the classification problem set up such as **size of the dataset, number of features and labels**.\n",
        "- `newton-cg`\n",
        "- `lbfgs`: default solver\n",
        "- `liblinear`\n",
        "- `sag`\n",
        "- `saga`\n",
        "\n",
        "- For **small datasets**, '**liblinear**' is a good choice, whereas '**sag**' and '**saga**' are faster for **large** ones.\n",
        "- For **unscaled datasets**, 'liblinear', 'lbfgs' and 'newton-cg' are robust.\n",
        "- For **multiclass problems**, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss.\n",
        "- 'liblinear' is limited to one-versus-rest schemes\n",
        "\n",
        "> By default, Logistic regression uses **L2 penalty**.\n",
        "\n",
        "- **Not all solvers supports all the penalties**.\n",
        "  - **L2 penalty** is supported by all solvers\n",
        "  - **L1 penalty** is supported only by a few solvers.  \n",
        "\n",
        "- C is specified in the constructor and must be positive\n",
        "  - **smaller value** leads to **stronger** regularization.\n",
        "  - **Larger value** leads to **weaker** regularization.  \n",
        "\n",
        "> `class_weight` parameter in the constructor of classifier estimators handles **class imbalance**.\n",
        "\n",
        "> **LogisticRegressionCV** implements logistic regression with in built **cross validation support** to find the **best values** of **C** and **l1_ratio** according to the specified **scoring** attribute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SGDClassifier\n",
        "- **SGD** is a simple yet very efficient **approach to fitting linear classifiers** under convex loss functions\n",
        "- It supports **multi-class classification** by combining multiple binary classifiers in a \"**one versus all**\" (OVA) scheme.\n",
        "- **Easily scales up to large scale problems** with more than $10^5$ training examples and $10^5$ features. It also works with **sparse** machine learning problems\n",
        "  - Text classification and natural language processing  \n",
        "\n",
        "We need to set **loss parameter** appropriately to build train classifier of our interest with **SGDClassifier**\n",
        "`loss` parameter:\n",
        "- `hinge`: (soft-margin) linear Support Vector Machine [By DEFAULT]\n",
        "- `log`: logistic regression\n",
        "- `modified_huber` - smoothed hinge loss brings tolerance to outliers as well as probability estimates\n",
        "- `squared_hinge`: like hinge but is quadratically penalized\n",
        "- `perceptron`: linear loss used by the perceptron algorithm\n",
        "- `squared_error`(least square classification), `huber`, `epsilon_insensitive`, or `squared_epsilon_insensitive` - regression losses\n",
        "\n",
        "|Advantages:|Disadvantages:|\n",
        "|---|---|\n",
        "|Efficiency|Requires a number of hyperparameters.|\n",
        "|Ease of implementation|Sensitive to feature scaling.|\n",
        "\n",
        "It is important to\n",
        "- **permute** (shuffle) the training data before fitting the model.\n",
        "- standardize the features.\n",
        "\n",
        "> By default: \n",
        "> - penalty='l2' and value is 0.0001\n",
        "> - max_iter = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO4v+ss6w7oDPW0sv7NVAa8",
      "include_colab_link": true,
      "name": "Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
