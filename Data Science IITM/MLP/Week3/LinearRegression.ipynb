{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEZYfG67aWde/IZokN7Nxf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjeesi/Notes-Notebooks/blob/master/Data%20Science%20IITM/MLP/Week3/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to build baseline regression model?\n",
        "`DummyRegressor` helps in creating **baseline** for regression.\n",
        "```python\n",
        "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
        "```\n",
        "Strategy\n",
        "- mean\n",
        "- median\n",
        "- quantile\n",
        "- constant\n",
        "\n",
        "# How is **Linear Regression** model trained?\n",
        "1. Instantiate **object** of suitable **linear regression estimator** from one of the following two options\n",
        "  - Normal Equation\n",
        "    ```python\n",
        "    linear_regressor = LinearRegression()\n",
        "    ```\n",
        "  - Iterative optimization\n",
        "    ```python\n",
        "    linear_regressor = SGDRegressor()\n",
        "    ```\n",
        "2. Call **fit** method on **linear regression object** with **training feature matrix** and **label vector** as arguments.\n",
        "  ```python\n",
        "  linear_regressor.fit(X_train, y_train)\n",
        "  ```\n",
        "  > Works for both single and multi-output regression.\n"
      ],
      "metadata": {
        "id": "5FjHtf-UDj-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGDRegressor Estimator\n",
        "- Use for large training set up (>10k samples)\n",
        "- provides **greator control on optimization process** through provision for **hyperparameter** settings:\n",
        "  - loss\n",
        "    - loss= 'squared error'\n",
        "    - loss= 'huber' (not heavily influenced by the **outliers**)  \n",
        "  - penalty=\n",
        "    - l1\n",
        "    - l2\n",
        "    - elasticnet\n",
        "  - learning_rate = \n",
        "    - constant\n",
        "    - optimal\n",
        "    - invscaling (default)\n",
        "    - adaptive\n",
        "  - early_stopping =\n",
        "    - True\n",
        "    - False\n",
        "\n",
        "*It's a good idea to use a* **random seed** *of your choice while instantiating SGDRegressor object. It helps us get* **reproducible results.**  \n",
        "\n",
        "Set `random_state` to seed of your choice.  \n",
        "\n",
        "## How to perform feature scaling for SGDRegressor\n",
        "SGD is **sensitive to feature scaling**, so it is **highly recommended to scale** input feature matrix.  \n",
        "```python\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sgd = Pipeline([\n",
        "    ('feature_scaling', StandardScaler()),\n",
        "    ('sgd_regressor', SGDRegressor())])\n",
        "sgd.fit(X_train, y_train)\n",
        "```\n",
        "> Note:\n",
        "> - Feature scaling **is not needed** for **word frequencies** and **indicator features** as they have intrinsic scale.\n",
        "> - Features extracted using PCA should be **scaled by some constant c** such that the average L2 norm of the training data equals one.\n",
        "\n",
        "## How to shuffle training data after each epoch in SGDRegressor?\n",
        "```python\n",
        "linear_reg = SGDRegressor(shuffle=True)\n",
        "```\n",
        "\n",
        "## How to use set **#epochs** in SGDRegressor?\n",
        "Set **max_iter** to desired **#epochs**. The default value is 100.  \n",
        "`linear_reg = SGDRegressor(max_iter=100)`\n",
        "\n",
        "> Remember **one epoch** is **one full pass over the training data**.\n",
        "\n",
        "Practical tip:  \n",
        "> SGD converges after observing approximately $10^6$ training samples. Thus, a reasonable first guess for the number of iterations for n sampled training set is:  \n",
        "`max_iter = np.ceil($10^6$/n)`\n",
        "\n",
        "## How to use set **stopping criteria** in SGDRegressor?\n",
        "Option 1: tol, n_iter_no_change, max_iter\n",
        "```python\n",
        "lin_reg = SGDRegressor( loss='squared_error',\n",
        "                        max_iter=500,\n",
        "                        tol=1e-3,\n",
        "                        n_iter_no_change=5)\n",
        "```\n",
        "Option 2: early_stopping, validation_fraction\n",
        "\n",
        "## How to use averaged SGD?\n",
        "Averaged SGD updates the weight vector to **average of weights** from previous updates.  \n",
        "Option 1: Averaging across all updates `average=True`  \n",
        "Option 2: Set `average` to int value.  \n",
        "> Averaged SGD works **best** with a **large number of features** and a **higher eta0** (initial leaning rate)\n",
        "\n",
        "## How do we initialize SGD with weight vector of the previous run?\n",
        "Set `warm_start = True`  \n",
        "By default `warm_start = False`\n",
        "\n",
        "## How to access the weights of trained **Linear Regression** model?\n",
        "$$\n",
        "\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_mx_m = \\textbf{w}^TX\n",
        "$$\n",
        "> The **weights** $w_1, w_2, ..., w_m$ are stored in `coef_` class variable.  \n",
        "`linear_regressor.coef_`  \n",
        "> The **intercept** $w_0$ is stored in `intercept_` class variable.\n",
        "`linear_regressor.intercept_`\n",
        "\n",
        "Note:\n",
        "  - These code snippets works for both **LinearRegression** and **SGDRegressor**, and for that matter to **all regression estimators**.\n",
        "\n",
        "## How to make predictions on new data in **Linear Regression** model?\n",
        "- Step 1: Arrange data for prediction in a feature matrix of shape(#samples, #features) or in sparse matrix format.\n",
        "- Step 2: Call **predict** method on **linear regression object** with **feature matrix** as an argument.\n",
        "  `linear_regressor.predict(X_test)`\n",
        "  > Same code works for **all regression estimators.**"
      ],
      "metadata": {
        "id": "EtYmdvHQKC-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "Steps-\n",
        "1. **Split data** into train and test\n",
        "2. **Fit** linear regression estimator on training set.\n",
        "3. **Calculate training error** (a.k.a. empirical error)\n",
        "4. **Calculate test error** (aka generalization error)\n",
        "5. **Compare** training and test errors\n",
        "\n",
        "## How to evaluate trained **Linear Regression** model?\n",
        "Using **score** method on **linear regression object**:\n",
        "```python\n",
        "# Evaluation on the eval set with\n",
        "# 1. feature matrix\n",
        "# 2. label vector or matrix (single/multi-output)\n",
        "linear_regressor.score(X_test, y_test)\n",
        "```\n",
        "The score returns *$R^2$* or coefficient of determination\n",
        "\n",
        "## Evaluation metrics\n",
        "- `mean_absolute_error`\n",
        "    - `train_error = mean_absolute_error(y_train, y_predicted)`\n",
        "- `mean_squarred_error`\n",
        "- `r2_score` also returns *$R^2$*\n",
        "\n",
        "### How to evaluate regression model on worst case error?\n",
        "Use metrics `max_error`  \n",
        "- can only be used for **single output regression**. It **does not support multi-output regression**.\n",
        "\n",
        "### Scores and Errors\n",
        "- Score is a metric for which higher value is better.\n",
        "- Error is a metric for which lower value is better.  \n",
        "\n",
        "Convert error to score by adding **neg_** suffix.\n",
        "- metrics.mean_absolute_error --> neg_mean_absolute\n",
        "\n",
        "### Cross-validation performs **robust evaluation** of model performance\n",
        "- by **repeated splitting** and\n",
        "- providing **many training and test errors**\n",
        "This enables us to **estimate variability in generalization performance** of the model.\n",
        "Cross-vaildation iterators:\n",
        "- `KFold`\n",
        "  - k: partitions\n",
        "  - k-1: training_set\n",
        "  - 1: test_set\n",
        "- `RepeatedKfold`\n",
        "- `LeaveOneOut`\n",
        "  - k = n(no. of training examples)\n",
        "  - 1 example for test_set\n",
        "- `ShuffleSplit`\n",
        "  - Random permutaions of shuffled data (does it repeatedly)\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "SIJok4Pj-aVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Kp6kcjPoQoZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}