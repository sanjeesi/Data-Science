{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preporcessing\n",
    "The real world training data is usually not clean and has many issues such as **missing values** for certain features, features on **different scales, non-numeric attributes** etc.  \n",
    "\n",
    "Often, there is a need to **pre-process** the data to make it amenable for training the model.\n",
    "\n",
    "> Sklearn provides a rich set of transformers for this job.  \n",
    "\n",
    "The ***same pre-processing** should be applied to both training and test set.  \n",
    "\n",
    "> Sklearn provides **pipeline** for making it easier to chain multiple transforms together and apply them *uniformly across train, eval and test sets*.\n",
    "\n",
    "Once you get the training data, the first job is to *explore the data* and list down **preprocessing** needed.  \n",
    "Typical problems include:\n",
    "* **Missing values** in features\n",
    "* Numerical features are *not on the same scale*.\n",
    "* **Categorical attributes** need to be represented with sensible numerical representation.\n",
    "* **Too many features**, reduce them.\n",
    "* **Extract features** from non-numeric data.\n",
    "\n",
    "Sklearn provides a *library of transformers* for *data preprocessing*.\n",
    "* Data cleaning (sklearn.preprocessing) such as *standardization*, *missing value imputation*, etc.\n",
    "* Feature extraction (sklearn.feature_extraction)\n",
    "* Feature reduction (sklearn.decomposition.pca)\n",
    "* Feature expansion (sklearn.kernal_approximation)\n",
    "\n",
    "**Transformer methods**\n",
    "Each transformer has the following methods:\n",
    "* `fit()` method learns model parameters from a training set.\n",
    "* `transform()` method applies the learnt transformation to the new data.\n",
    "* `fit_transform()` method performns both `fit()` and `transform()` methods and is more convenient and efficient to use.\n",
    "\n",
    "## Part 1: Feature extraction\n",
    "`sklearn.feature_extraction` has useful APIs to extract features from data:\n",
    "* `DictVectorizer`\n",
    "* `FeatureHasher`\n",
    "\n",
    "### `DictVectorizer`  \n",
    "Converts lists of mappings (dict-like objects) of feature names to feature values, into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [2., 1., 0.],\n",
       "       [1., 3., 2.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "X = dv.fit_transform([{'height': 1, 'length': 0, 'width': 1}, \\\n",
    "    {'height': 2, 'length': 1, 'width': 0}, \\\n",
    "    {'height': 1, 'length': 3, 'width': 2}])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'height': 1.0, 'width': 1.0},\n",
       " {'height': 2.0, 'length': 1.0},\n",
       " {'height': 1.0, 'length': 3.0, 'width': 2.0}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `FeatureHasher`\n",
    "* High-speed, low-memory vectorizer that uses feature hashing technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction from images and text\n",
    "* `sklearn.feature_extraction.image.*` has useful APIs to extract features from images data.\n",
    "* `sklearn.feature_extraction.text.*` has useful APIs to extract features from text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Data Cleaning\n",
    "### 2.1 Handling missing values\n",
    "* Many ML algorithms do not work with missing data and need all features to be present.  \n",
    "* Discarding records containing missing values would result in loss of valuable training samples.\n",
    "> `sklearn.impute` API provides functionality to fill missing values in a dataset.\n",
    "  * `SimpleImputer`\n",
    "  * `KNNImputer`\n",
    "\n",
    "> `MissingIndicator` provides indicators for missing values.\n",
    "\n",
    "#### SimpleImputer\n",
    "* Fills missing values with one of the following strategies:\n",
    "  * mean, median, most_frequent and constant.\n",
    "\n",
    "#### KNNImputer\n",
    "* Uses k-nearest neighbours approach to fill missing values in a dataset.\n",
    "  * filled with the **mean** value of the same attribute of `n_neighbhors` closest neighbours.\n",
    "* The nearest neighbours are determined using the **Euclidean distance**.\n",
    "\n",
    "```python\n",
    "knnImputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
    "knnImputer.fit_transform(X)\n",
    "```\n",
    ">By default, the `KNNImputer` uses `mean` strategy.\n",
    "\n",
    "#### Marking imputed values\n",
    "* It is important to indicate the *presence of missing values* in the dataset.\n",
    "* `MissingIndicator` helps us to get those indications.\n",
    "  * It returns a **binary matrix**,\n",
    "    * **True values** correspond to missing entries in the original dataset.\n",
    "\n",
    "### 2.2 Numeric transformers\n",
    "1. Feature scaling\n",
    "2. Polynomial transformation\n",
    "3. Discretization\n",
    "\n",
    "#### Feature scaling\n",
    "Numerical features with different scales leads to slower convergence of iterative optimization procedures.  \n",
    "It is a good practice to scale numerical features so that all of them are on the same scale.  \n",
    "\n",
    "Three feature scaling APIs are available in sklearn:\n",
    "* `StandardScaler`\n",
    "* `MinMaxScaler`\n",
    "* `MaxAbsScaler`\n",
    "\n",
    "`StandardScaler`  \n",
    "Transforms the original feature vector x into new feature vector x' using following formula:  \n",
    "> $$\n",
    "x' = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "After standardization, the trasformed feature vector $x'$ will have mean $(\\mu) = 0$ and standard deviation $(\\sigma) = 1$.  \n",
    "\n",
    "`MinMaxScaler`  \n",
    "Transforms to x' so that all values fall within range [0, 1].\n",
    "> $$\n",
    "x' = \\frac{x - x.\\min}{x.\\max - x.\\min}\n",
    "$$\n",
    "\n",
    "`MaxAbsScaler`  \n",
    "Transforms to x' so that all values fall within range [-1, 1].  \n",
    "> $$\n",
    "x' = \\frac{x}{\\max(|x|)}\n",
    "$$\n",
    "\n",
    "`FunctionalTransformer`  \n",
    "Constructs transformed features by applying **a user defined function** to the original features.\n",
    "```python\n",
    "ft = FunctionTransformer(numpy.log2)\n",
    "ft.fit_transform(X)\n",
    "```\n",
    "\n",
    "#### Polynomial transformation\n",
    "Generates a new feature matrix consisting of **all polynomial combinations** of the features with **degree less than or equal to the specified degree**.  \n",
    "```python\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "X = [a, b]\n",
    "pf.fit_transform(X)\n",
    "```\n",
    "$$\n",
    "X' = [1, a, b, a^2, ab, b^2,]\n",
    "$$\n",
    "\n",
    "```python\n",
    "pf = PolynomialFeatures(degree=3)\n",
    "X = [a, b]\n",
    "pf.fit_transform(X)\n",
    "```\n",
    "$$\n",
    "X' = [1, a, b, ab, a^2, b^2, a^2b, ab^2, a^3, b^3]\n",
    "$$\n",
    "\n",
    "#### `KBinsDiscretizer`\n",
    "* Divides a continuous feature into bins.\n",
    "* *One hot encoding* or *ordinal encoding* is further applied to the bin labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "kbins = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(9)\n",
    "X = X*0.125\n",
    "X = X.reshape(9,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [4.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbins.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Categorical transformers\n",
    "1. Feature encoding\n",
    "2. Label encoding\n",
    "\n",
    "### `OneHotEncoder`\n",
    "* Encodes categorical feature or label as a *one-hot numeric array*.\n",
    "* Creates **one binary column* for each of *K unique values*.\n",
    "* **Exactly one column* has 1 in it and the rest have 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [1]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "x = np.array([1, 2, 3, 1]).reshape(-1, 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "x_tr = ohe.fit_transform(x)\n",
    "x_tr.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Number of unique values in [1, 2, 3, 1]:*  \n",
    "K = 3  \n",
    "\n",
    "*Number of columns in transformed matrix* = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `LabelEncoder`\n",
    "Encodes target labels with value between 0 and K-1, where K is number of distinct values.\n",
    "\n",
    "### `OrdinalEncoder`\n",
    "Encodes categorical features with value between 0 and K-1, where K is number of distinct values.  \n",
    "\n",
    "> OrdinalEncoder can operate **multi dimensijonal data*, while LabelEncoder can transform *only 1D data*.\n",
    "\n",
    "### `LabelBinarizer`\n",
    "Several regression and binary classification can be extended to multi-class setup in **one-vs-all** fashion.  \n",
    ">This involves training a single regressor or classifier per class.  \n",
    "For this, we need to **convert multi-class labels to binary labels** and **LabelBinarizer** performs this task.  \n",
    "\n",
    "*If estimator supports multiclass data, LabelBinarizer is not needed.*\n",
    "\n",
    "### `MultiLabelBinarizer`\n",
    "* Encodes **categorical features** with value between 0 and K-1, where K is number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `add_dummy_feature`\n",
    "**Augments** dataset with **a column vector**, each value in the column is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature selection\n",
    "- Filter based\n",
    "- Wrapper based\n",
    "\n",
    "- Sometimes in real world dataset, **all features do not contribute well enough towards fitting a model**.\n",
    "- The features that do not contribute significantly, can be **removed**. It leads to **decrease in size of the dataset** and hence, the **computation cost** of fitting a model.\n",
    "- `sklearn.feature_selection` provides many APIs to accomplish this task.\n",
    "\n",
    "### Filter\n",
    "- Variance threshold\n",
    "- SelectKBest\n",
    "- SelectPercentile\n",
    "- GenericUnivariateSelect\n",
    "\n",
    "### Wrapper\n",
    "- RFE\n",
    "- RFECV\n",
    "- SelectFromModel\n",
    "- SequentialFeatureSelector\n",
    "\n",
    "### Filter based feature selection methods\n",
    "#### Variance threshold\n",
    "**Removes** all features with **variance below a certain threshold**, as specified by the user, from input feature matrix.\n",
    "> By default removes a feature which has same value, i.e. zero variance.\n",
    "\n",
    "### Univariate feature selection\n",
    "Univariate feature selection **selects** features based on univariate statistical tests.  \n",
    "\n",
    "There are three APIs for univariate feature selection:\n",
    "* `SelectKBest`: Removes **all but** the **k highest scoring** features\n",
    "* `SelectPercentile`: Removes **all but** a user-specified **highest scoring percentage** of features\n",
    "* `GenericUnivariateSelect`: Performs univariate feature selection with a **configurable strategy**, which can be found via **hyper-parameter search**.\n",
    "\n",
    "sklearn provides one more class of univariate feature selection methods that work on **common univariate statistical tests** for each feature:\n",
    "- **SelectFpr** selects features based on a false positive rate test.\n",
    "- **SelectFdr** selects features based on an estimated false discovery rate.\n",
    "- **SelectFwe** selects features based on family-wise error rate.\n",
    "\n",
    "### Univariate scoring function\n",
    "* Each API need a **scoring function** to score each feature.\n",
    "* Three classes of scoring functions are proposed:\n",
    "  - Mutual information (MI)\n",
    "  - Chi-square\n",
    "  - F-statistics\n",
    "* MI and F-statistics can be used in both **classification** and **regression** problems.\n",
    "  - `mutual_info_regression`\n",
    "  - `mutual_info_classif`\n",
    "  - `f_regression`\n",
    "  - `f_classif`\n",
    "* Chi-square can be used only in **regression** problem.\n",
    "  - `chi2`\n",
    "\n",
    "#### Mutual Information (MI)\n",
    "- **Measures dependency** between two variables.\n",
    "- It returns a **non-negative** value.\n",
    "  - MI = 0 for **independent* variables.\n",
    "  - Higher MI indicates **higher dependency**.\n",
    "\n",
    "#### Chi-square\n",
    "* **Measures dependence** between two variables.\n",
    "* Computes chi-square stats between **non-negative feature** (boolean or frequencies) and **class label**.\n",
    "* **Higher chi-square values** indicates that the **feature and labels are likely to be correlated**. Hence we choose to include such features with higher chi-square value.\n",
    "\n",
    "*Mi and chi-square feature selection is recommended for **sparse data***.  \n",
    "\n",
    "#### GenericUnivariateSelect\n",
    "```python\n",
    "transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n",
    "X_new = transformer.fit_transform(X, y)\n",
    "```\n",
    "- Selects 20 best features based on chi-square scoring function\n",
    "- Selects set of features based on a feature selection mode and a scoring function.\n",
    "- The `mode` could be `percentile` (default), `k_best`, `fpr`, `fdr`, `fwe`.\n",
    "- The `param` argument takes value corresponding to the `mode`.\n",
    "\n",
    "> *Do not use regression feature scoring function with classification problem. It will lead to useless results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper based feature selection\n",
    "Unlike filter based feature selection, wrapper based methods use **estimator class** rather than a **scoring function**.\n",
    "#### Recursive Feature Elimination (RFE)\n",
    "- Uses an **estimator** to **recursively remove features**.\n",
    "  - Initially fits an estimator on all features.\n",
    "  - Obtains **feature importances** from the estimator and **removes one or more least important features** (depending upon `step` parameter).\n",
    "  - Repeats the process **until the specified number of features** is reached.\n",
    "\n",
    "> - Use `RFECV` if we do not want to specify the desired number of features in `RFE`.\n",
    "> - It performs `RFE` in a **cross-validation loop** to find the **optimal number of features**.\n",
    "\n",
    "#### `SelectFromModel`\n",
    "First trains an estimator on all features, then it selects a desired number of features (as specified with **max_features** parameter) above **certain threshold of feature importance**.\n",
    "\n",
    "```python\n",
    "clf = LinearSVC(C=0.01, penalty=\"l1\", dual=False)\n",
    "clf = clf.fit(X, y)\n",
    "clf.coef_\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "```\n",
    "- Here we use a linear support vector classifier to get coefficients of the features for `SelectFromModel` transformer.\n",
    "- **L1 regularizer** essentially gets us **non-zero weights only to features that are useful**, all other weights are zero.\n",
    "- It ends up selecting features with non-zero weights or coefficients.\n",
    "\n",
    "#### Sequential feature selection\n",
    "Performs feature selection by **selecting or deselecting features one by one in a greedy manner**.\n",
    "- Forward selection:\n",
    "  - Starting with zero feature, it finds one feature that obtains the best cross validation score for an estimator when trained on that feature.\n",
    "  - Repeats the process by adding a new feature to the set of selected features.\n",
    "- Backward selection:\n",
    "  - Starting with all features and removes least important features one by one following the idea of forward selection.\n",
    "Stops when reach the desired number of features.  \n",
    "\n",
    "- In general, forward and backward selection **do not yield equivalent results**.\n",
    "- Select the direction that is **efficient** for the required number of selected features.\n",
    "- SFS does not require the underlying model to expose a `coef_` or `feature_importances_` attribute unlike in `RFE` and `SelectFromModel`.\n",
    "- SFS may be slower than `RFE` and `SelectFromModel` as it needs to evaluate more models compared to the other two approaches.\n",
    "\n",
    "For example in backward selection, the iteration going from **m** features to **m-1** features using k-fold CV requires fitting m x k models, while\n",
    "  - `RFE` would require only a single fit, and\n",
    "  - `SelectFromModel` performs a single fit and requires no iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous features transformations\n",
    "Generally training data contains diverse features such as numerical and categorical.  \n",
    "\n",
    "Different feature types are processed with different transformers.  \n",
    "\n",
    "Need a way to combine different feature transformers seamlessly.\n",
    "`sklearn.compose` has useful classes and methods to apply transformation on subset of features and combine them:\n",
    "- `ColumnTransformer`\n",
    "  - It applies **a set of transformers** to columns of an array or `pandas.DataFrame`, **concatenates the transformed outputs** from different tranformers into a **single matrix**.\n",
    "  - It is useful for **transforming heterogenous data** by applying **different transfrormers to separate subsets of features**.\n",
    "  - It combines different feature selection mechanisms and transformation into a single transformer object.\n",
    "  - Each tuple has format\n",
    "    - ```(transformerName, transformer(), columnIndices)```\n",
    "\n",
    "- `TransformedTargetRegressor`\n",
    "  - Transforms the **target variable** `y` **before fitting** a regression model.\n",
    "  - The predicted values are mapped back to the original space via an inverse transform.\n",
    "  - `TransformedTargetRegressor` takes `regressor` and `transformer` to be applied to the target variable as arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Dimensionality reduction\n",
    "Another way to reduce the number of feature is through **unsupervised dimensionality reduction** techniques.  \n",
    "`sklearn.decomposition` model has a number of APIs for this task.  \n",
    "> We will focus on how to perform feature reduction with **Principal Component Analysis (PCA)** in sklearn.\n",
    "\n",
    "### PCA 101\n",
    "- **PCA**, is a linear dimensionality reduction technique.\n",
    "- It uses **singular value decomposition (SVD)** to project the feautre matrix or data to a **lower dimensional space**.\n",
    "- The first principal component (PC) is in the **direction of maximum variance** in the data.\n",
    "  - It captures **bulk of the variance** in the data.\n",
    "- The **subsequent PCs** are **orthogonal** to the first PC and **gradually capture lesser and lesser variance** in the data.\n",
    "- We can **select first k PCs** such that we are able to capture the **desired variance** in the data.\n",
    "\n",
    "> `sklearn.decomposition.PCA` API is used for performing PCA based dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Chaining Transformers\n",
    "The preprocessing transformations are applied one after another on the input feature matrix.  \n",
    "\n",
    "> It is important to apply **exactly same transformation** on training, evaluation and test set **in the same order**.\n",
    "\n",
    "Failing to do so would lead to **incorrect predictions** from model due to **distribution shift** and hence **incorrect performance evaluation**.  \n",
    "\n",
    "The `sklearn.pipeline` module provides utilities to build a **composite estimator**, as a **chain of transformers and estimators**.  \n",
    "There are two classes:\n",
    "1. Pipeline\n",
    "   - Constructs a chain of multiple transformers to execute a fixed sequence of steps in data preprocessing and modelling.\n",
    "2. FeatureUnion\n",
    "   - Combines output from several transformer objects by creating a new transformer from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing individual steps in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (531685235.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [2]\u001b[0;36m\u001b[0m\n\u001b[0;31m    form sklearn.model import LinearRegression\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "estimator = [\n",
    "    ('simpleImputer', SimpleImputer()),\n",
    "    ('pca', PCA()),\n",
    "    ('regressor', LinearRegression())\n",
    "]\n",
    "pipe = Pipeline(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
