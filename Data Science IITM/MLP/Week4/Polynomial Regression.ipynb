{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "1. Apply **polynomial transformation** on the feature matrix.\n",
    "2. Learn **linear regression model** on the **transformed feature matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use **only interaction features** for polynomial regression?\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_trasform = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "```\n",
    "Here:  \n",
    "> [$x_1, x_2$] is transformed to [$1, x_1, x_2, x_1x_2].  \n",
    "> Note that [$x_1^2, x_2^2$] are excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "### 1. Ridge regularization\n",
    "> a.k.a. L2 regularization\n",
    "#### How to search the **best regularization** parameter for **ridge**?\n",
    "- Option #1:\n",
    "  Search for the best regularization rate with built-in cross validation in **RidgeCV** estimator.\n",
    "\n",
    "- Option #2:\n",
    "  Use cross validation with **Ridge** or **SVDRegressor** to search for best regularization.\n",
    "  - Grid search\n",
    "  - Randomized search\n",
    "\n",
    "### 2. Lasso regularization\n",
    "> a.k.a. L1 regularization\n",
    "- The **best regularization** parameter can be found in similar manner as in ridge.\n",
    "\n",
    "### 3. Elasticnet regularization\n",
    "> convex combination of L1 (lasso) and L2 (ridge)\n",
    "- If we set `l1_ratio = 0.3`,  \n",
    "  it means `l2_ratio = 1 - l1_ratio = 0.7`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning (HPT)\n",
    "### How to recognize hyperparameters in any sklearn estimator?\n",
    "- **Hyper-parameters** are parameters that are **not directly learnt** within estimators.\n",
    "- In `sklearn`, they are passed as **arguments** to the **constructor** of the **estimator** classes.\n",
    "For example,\n",
    "  - degree in PolynomialFeatures\n",
    "  - learning rate in SGDRegressor\n",
    "\n",
    "### How to set these hyperparameters?\n",
    "Select hyperparameters that results in the **best cross validation scores**.  \n",
    "\n",
    "Hyper parameter search consists of\n",
    "- an estimator (regressor or classifier);\n",
    "- a parameter space;\n",
    "- a method for searching or sampling candidates;\n",
    "- a cross-validation scheme; and\n",
    "- a score function\n",
    "\n",
    "#### Two generic HTP approaches:\n",
    "|`GridSearchCV`|`RandomizedSearchCV`|\n",
    "|--------------|--------------------|\n",
    "|exhaustively considers all parameter combinations for specified values.|samples a given number of candidate values from a parameter space with a specified distribution.|\n",
    "|specifies exact values of parameters in grid|specifies distributions of parameter values and values are smapled from those distributions.|\n",
    "||Computational budget can be chosen independent of number of parameters and their possible values.\n",
    "||The budget is chosen in terms of the number of sampled candidates or the number of training iterations. Specified in `n_iter` argument|\n",
    "\n",
    "### Steps in HTP\n",
    "1. Divide training data into training, validation and test sets.\n",
    "2. For each combinations of hyper-parameter values learn a model with training set.  \n",
    "  *This step creates multiple models.*  \n",
    "  \n",
    "  Tips:\n",
    "  - This step can be run **in parallel** by setting `n_jobs= -1`.\n",
    "  - Some parameter combinations may cause failure in fitting one or more folds of data. This may cause the search to fail. Set `error_score = 0` (or np.NAN) to set score for the problematic fold to 0 and complete the search.\n",
    "\n",
    "3. **Evaluate performance** of each model with validation and select a model with the best evaluation score.\n",
    "4. Retain model with the best hyper-parameter settings on training and validation set combined.\n",
    "5. Evaluate the model performance on the test set.  \n",
    "  *Note that the test set was not used in hyper-parameter search and model training.*\n",
    "\n",
    "### What are some of model specific HPT available for regression task?\n",
    "- Some models can fit data for a **range of values of some parameter** almost **as efficiently as** fitting the estimator for a **single value** of the parameter.\n",
    "- This feature can be leveraged to perfom **more efficient cross-validation** used for model selection of this parameter.\n",
    "  - linear_model.LassoCV\n",
    "  - linear_model.RidgeCV\n",
    "  - linear_model.ElasticNetCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
