{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qcCHsx6wE-Y"
   },
   "source": [
    "# Decision Trees  \n",
    "Decision Trees are **non-parametric supervised learning** methods that can be used for modelling **classification** as well as **regression** problems. \n",
    "\n",
    "> partitions the feature space into a set of rectangles (or cuboid) and then fit a simple model (like a constant) in each one.\n",
    "\n",
    "1. Binary Decision tree\n",
    "  - splits into two branches at each node.  \n",
    "\n",
    "Important Issues:\n",
    "1. Which attribute to choose for splitting?\n",
    "2. What should be the splitting criterion?\n",
    "3. What tree size will give the optimal solution?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WnMFlT4vrax"
   },
   "source": [
    "**ID3 (Iterative Dichotomizer 3) Algorithm for Decision Trees**\n",
    "![ID3 tree.png](https://drive.google.com/uc?id=1ugdBXPEiXgBkKGXYnmkOiFFfCd9T5AmQ)  \n",
    "\n",
    "Recursion on a subset may stop by any of the **Stopping criteria**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZgXhMnuDHsQ"
   },
   "source": [
    "- Impurity Measures\n",
    "  - Proportion of data in node i,  \n",
    "  $p_{i,k} = \\frac{1}{N _{i}} \\sum \\limits _{x ^{(i)} \\in R _{i}} 1(y ^{(i)} = k) $ &emsp; &emsp; _$N _{i}$ is number of samples in region $R _{i}$_\n",
    "  - Misclassification Error\n",
    "  - Gini Index\n",
    "  - Entropy, (means **randomness**)  \n",
    "  $H_{i} = - \\sum \\limits _{k=1} ^{n} p _{i,k} log _{2} p _{i,k}$ \n",
    "  - Information gain\n",
    "  ![Information gain formula](https://drive.google.com/uc?id=13_8cr777OUcJjdEEVz4w70728WhnViqy)\n",
    "\n",
    "In ID3, information gain can be calculated (instead of entropy) for each remaining attribute. The attribute with the **largest** information gain is used to split the dataset on this iteration.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKRm0B0DuMM9"
   },
   "source": [
    "### CART  \n",
    "- CART can model both **regression** and **classification** problems.  \n",
    "- CART learns **decision trees** for **classification** very similar to **ID3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SriiK4cE3jEk"
   },
   "source": [
    "### Overfitting in Decision Trees  \n",
    "A very large tree might overfit the data.\n",
    "\n",
    "Methods to avoid overfitting:\n",
    "- Pre-pruning\n",
    "- Cost-complexity pruning  \n",
    "- Post-pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNTTnhLyR7WH"
   },
   "source": [
    "### Advantages of Decision Trees:\n",
    "- Versatile, can perform classification, regression and multi-output tasks.\n",
    "- Simple to understand and interpret. Trees can be visualised.\n",
    "- Needs little data preprocessing.\n",
    "- Cost of using tree is logarithmic in the number of training data samples.  \n",
    "\n",
    "### Disadvantages of Decision Trees:\n",
    "- Can create overly complex trees that do not generalize to unseen data. Pruning is needed to address this issue.\n",
    "- Decision trees suffer from the problem of high variance, which can be mitigated by ensembles.\n",
    "- Trees learn peicewise linear approximation and hence are not good at extrapolation.\n",
    "\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGSZok6eULyi"
   },
   "source": [
    "## Implementation of Decision tree for classification with ID3 algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-abBMfiUsDN"
   },
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XmlIbLgU7pN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cmJfGglVB6U",
    "outputId": "6f9adfcf-ebae-43df-9fd6-54405d03c71b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = np.finfo(float).eps\n",
    "eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6N3sEKbVVz3"
   },
   "source": [
    "Here `eps` is the smallest representable number. At times we get `log(0)` or `0` in the denominator, to avoid that we are going to use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAHjRsLBV4O7"
   },
   "source": [
    "#### Demo: Classification\n",
    "In this case we will use a synthetic data set for classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDHjtDeHp4FZ",
    "outputId": "de4e4829-818c-4993-fd80-ac0d8a0ccbb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#@title [Load Dataset]\n",
    "\n",
    "# mount Google drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "\n",
    "# read file\n",
    "path = \"/content/drive/MyDrive/Colab Notebooks/Datasets/tennis.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nULSfkqnNYXa",
    "outputId": "334eeaf8-faac-4881-eeab-4ced8a65247c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "deEkfIn6OlWa",
    "outputId": "5846769b-89b5-47f8-ce4a-cfe9f84e34d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['sunny', 'hot', 'high', False, 'no'],\n",
       "       ['sunny', 'hot', 'high', True, 'no'],\n",
       "       ['overcast', 'hot', 'high', False, 'yes'],\n",
       "       ['rainy', 'mild', 'high', False, 'yes'],\n",
       "       ['rainy', 'cool', 'normal', False, 'yes'],\n",
       "       ['rainy', 'cool', 'normal', True, 'no'],\n",
       "       ['overcast', 'cool', 'normal', True, 'yes'],\n",
       "       ['sunny', 'mild', 'high', False, 'no'],\n",
       "       ['sunny', 'cool', 'normal', False, 'yes'],\n",
       "       ['rainy', 'mild', 'normal', False, 'yes'],\n",
       "       ['sunny', 'mild', 'normal', True, 'yes'],\n",
       "       ['overcast', 'mild', 'high', True, 'yes'],\n",
       "       ['overcast', 'hot', 'normal', False, 'yes'],\n",
       "       ['rainy', 'mild', 'high', True, 'no']], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJjKl2rtUAVk",
    "outputId": "e124092a-a90a-404c-fb68-17fd4671dcdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['outlook', 'temp', 'humidity', 'windy', 'play'], dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MN1hkxZQU4_l",
    "outputId": "3f0370c5-832f-4d3e-d5a5-b17837b013a6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df.keys()[-1]\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vpNU_RgUVbA"
   },
   "source": [
    "Lets check total number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kn8IMnOOUVYn",
    "outputId": "bbbdbc6d-04e8-4935-fb99-c344351c4337"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['no', 'yes'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[target].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pqrt54R6UVWD",
    "outputId": "105de27f-694d-4654-e070-8315170e85f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Number of 'no' and 'yes'\n",
    "print(df[target].value_counts()[df[target].unique()[0]])\n",
    "print(df[target].value_counts()[df[target].unique()[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7kyok-bUVTf"
   },
   "source": [
    "### Calculating Entropy of the whole dataset  \n",
    "Since the formula for **information gain** requires the entropy of the whole dataset, we compute that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jwNPtpmUVNL",
    "outputId": "8ed75b9b-d499-42c2-92ec-88829b7834ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9402859586706309"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findEntropyWhole(df):\n",
    "  #last column in the dataframe is the target variable\n",
    "  target = df.keys()[-1]\n",
    "\n",
    "  #initailization\n",
    "  overall_entropy = 0\n",
    "\n",
    "  #possible values of target\n",
    "  values_in_target = df[target].unique()\n",
    "\n",
    "  for value in values_in_target:\n",
    "    p = df[target].value_counts()[value]/len(df[target])\n",
    "    overall_entropy += -p*np.log2(p)\n",
    "  return overall_entropy\n",
    "\n",
    "findEntropyWhole(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rYoImmgUU_K"
   },
   "source": [
    "###Calculating entropy of an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMKbcl3bZVIl"
   },
   "outputs": [],
   "source": [
    "def findEntropyAttribute(df, attribute):\n",
    "  #last column in the dataframe is the target variable\n",
    "  target = df.keys()[-1]\n",
    "\n",
    "  #initailization\n",
    "  overall_entropy = 0\n",
    "\n",
    "  #possible values of target\n",
    "  values_in_target = df[target].unique()\n",
    "\n",
    "  #this gives different features in that attribute like hot, cold in temperature\n",
    "  values_in_attribute = df[attribute].unique()\n",
    "\n",
    "  #initialize attribute entropy\n",
    "  entropy_attribute = 0\n",
    "\n",
    "  for value_in_attribute in values_in_attribute:\n",
    "    overall_entropy = 0\n",
    "    for value_in_target in values_in_target:\n",
    "      num = len(df[attribute][df[attribute] == value_in_attribute][df[target] == value_in_target])\n",
    "      den = len(df[attribute][df[attribute] == value_in_attribute])\n",
    "      p = num/(den+eps)\n",
    "      overall_entropy += -p*np.log2(p+eps)\n",
    "    p2 = den/len(df)\n",
    "    entropy_attribute += -p2*overall_entropy\n",
    "  return abs(entropy_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5GB1LF5ZVGS"
   },
   "source": [
    "Let's compute entropy of different attributes now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4sKgUTBZVD7",
    "outputId": "1ffddc30-f816-4c8d-ea35-9e619adf248a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of attribute outlook: 0.6935361388961914\n",
      "Entropy of attribute temp: 0.9110633930116756\n",
      "Entropy of attribute humidity: 0.7884504573082889\n",
      "Entropy of attribute windy: 0.892158928262361\n"
     ]
    }
   ],
   "source": [
    "for attribute in df.keys()[:-1]:\n",
    "  print('Entropy of attribute {}: {}'.format(attribute, findEntropyAttribute(df, attribute)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOQw6FNYZVBl"
   },
   "source": [
    "Finding the best attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "r5pAE4aOZU_O",
    "outputId": "e333a861-04eb-425f-84eb-4605385257b9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'outlook'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_best_attribute_to_divide(df):\n",
    "  #Information gain\n",
    "  IG = []\n",
    "\n",
    "  # all column names\n",
    "  all_attributes_names = df.keys()[:-1]\n",
    "\n",
    "  for attribute in all_attributes_names:\n",
    "    # compute information gain for every attribute\n",
    "    IG.append(findEntropyWhole(df)- findEntropyAttribute(df, attribute))\n",
    "\n",
    "    # get index of attribute with best IG\n",
    "    max_IG_attribute_index = np.argmax(IG)\n",
    "\n",
    "    # best attribute name\n",
    "    best_attribute = all_attributes_names[max_IG_attribute_index]\n",
    "  return best_attribute\n",
    "\n",
    "find_best_attribute_to_divide(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZDHKG7WZU84"
   },
   "source": [
    "#### Building the Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9w1MjDDxZU6U",
    "outputId": "53920c94-a6ba-45cc-e658-9b34670d2dcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outlook': {'overcast': 'yes',\n",
       "  'rainy': {'windy': {False: 'yes', True: 'no'}},\n",
       "  'sunny': {'humidity': {'high': 'no', 'normal': 'yes'}}}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_tree(df, tree=None):\n",
    "  # last column in dataframe\n",
    "  target = df.keys()[-1]\n",
    "\n",
    "  # Here we build our Decision tree\n",
    "\n",
    "  # get attribute with max IG\n",
    "  node = find_best_attribute_to_divide(df)\n",
    "\n",
    "  # get distinct values of that attribute\n",
    "  attributeValues = np.unique(df[node])\n",
    "\n",
    "  # create an empty dictionary to create tree\n",
    "  if tree is None:\n",
    "    tree = {}\n",
    "    tree[node] = {}\n",
    "\n",
    "  # we make loop to construct a tree by calling this function recursively\n",
    "  # in this we check if the subset is pure and stops if it is pure\n",
    "  for value in attributeValues:\n",
    "    subtable = df[df[node] == value].reset_index(drop=True)\n",
    "    clValue, counts = np.unique(subtable['play'], return_counts=True)\n",
    "    if len(counts) == 1: # Checking purity of subset\n",
    "      tree[node][value] = clValue[0]\n",
    "    else:\n",
    "      tree[node][value] = build_tree(subtable) # calling the function recursively\n",
    "  return tree\n",
    "\n",
    "build_tree(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gm0g2TWxnlXl"
   },
   "source": [
    "ID3 in its pure form performs no backtracking in its search. Once it selects an attribute to test at a particular level in the tree, it never backtracks to reconsider this choice. Therefore, it is susceptible to the usual risk of hill-climbing search without backtracking: converging to local optimal solutions that may not be globally optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFIPFfEGqHTM"
   },
   "source": [
    "### Interview questions\n",
    "1. Entropy vs Gini Index vs IG\n",
    "2. When to use what?\n",
    "3. Decision tree in Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LuPZn-7eT97R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(0, 5)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [range(5)]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Decision_Trees.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
