{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qcCHsx6wE-Y"
      },
      "source": [
        "#Decision Trees  \n",
        "Decision Trees are **non-parametric supervised learning** methods that can be used for modelling **classification** as well as **regression** problems. \n",
        "\n",
        "> partitions the feature space into a set of rectangles (or cuboid) and then fit a simple model (like a constant) in each one.\n",
        "\n",
        "1. Binary Decision tree\n",
        "  - splits into two branches at each node.  \n",
        "\n",
        "Important Issues:\n",
        "1. Which attribute to choose for splitting?\n",
        "2. What should be the splitting criterion?\n",
        "3. What tree size will give the optimal solution?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WnMFlT4vrax"
      },
      "source": [
        "**ID3 (Iterative Dichotomizer 3) Algorithm for Decision Trees**\n",
        "![ID3 tree.png](https://drive.google.com/uc?id=1ugdBXPEiXgBkKGXYnmkOiFFfCd9T5AmQ)  \n",
        "\n",
        "Recursion on a subset may stop by any of the **Stopping criteria**.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZgXhMnuDHsQ"
      },
      "source": [
        "- Impurity Measures\n",
        "  - Proportion of data in node i,  \n",
        "  $p_{i,k} = \\frac{1}{N _{i}} \\sum \\limits _{x ^{(i)} \\in R _{i}} 1(y ^{(i)} = k) $ &emsp; &emsp; _$N _{i}$ is number of samples in region $R _{i}$_\n",
        "  - Misclassification Error\n",
        "  - Gini Index\n",
        "  - Entropy, (means **randomness**)  \n",
        "  $H_{i} = - \\sum \\limits _{k=1} ^{n} p _{i,k} log _{2} p _{i,k}$ \n",
        "  - Information gain\n",
        "  ![Information gain formula](https://drive.google.com/uc?id=13_8cr777OUcJjdEEVz4w70728WhnViqy)\n",
        "\n",
        "In ID3, information gain can be calculated (instead of entropy) for each remaining attribute. The attribute with the **largest** information gain is used to split the dataset on this iteration.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CART  \n",
        "- CART can model both **regression** and **classification** problems.  \n",
        "- CART learns **decision trees** for **classification** very similar to **ID3**.\n",
        "\n"
      ],
      "metadata": {
        "id": "BKRm0B0DuMM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfitting in Decision Trees  \n",
        "A very large tree might overfit the data.\n",
        "\n",
        "Methods to avoid overfitting:\n",
        "- Pre-pruning\n",
        "- Cost-complexity pruning  \n",
        "- Post-pruning"
      ],
      "metadata": {
        "id": "SriiK4cE3jEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of Decision Trees:\n",
        "- Versatile, can perform classification, regression and multi-output tasks.\n",
        "- Simple to understand and interpret. Trees can be visualised.\n",
        "- Needs little data preprocessing.\n",
        "- Cost of using tree is logarithmic in the number of training data samples.  \n",
        "\n",
        "### Disadvantages of Decision Trees:\n",
        "- Can create overly complex trees that do not generalize to unseen data. Pruning is needed to address this issue.\n",
        "- Decision trees suffer from the problem of high variance, which can be mitigated by ensembles.\n",
        "- Trees learn peicewise linear approximation and hence are not good at extrapolation.\n",
        "\n",
        "\n",
        "___"
      ],
      "metadata": {
        "id": "TNTTnhLyR7WH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Decision tree for classification with ID3 algorithm:"
      ],
      "metadata": {
        "id": "SGSZok6eULyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing libraries"
      ],
      "metadata": {
        "id": "k-abBMfiUsDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "4XmlIbLgU7pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps = np.finfo(float).eps\n",
        "eps"
      ],
      "metadata": {
        "id": "4cmJfGglVB6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f9adfcf-ebae-43df-9fd6-54405d03c71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.220446049250313e-16"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here `eps` is the smallest representable number. At times we get `log(0)` or `0` in the denominator, to avoid that we are going to use this."
      ],
      "metadata": {
        "id": "K6N3sEKbVVz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Demo: Classification\n",
        "In this case we will use a synthetic data set for classification task."
      ],
      "metadata": {
        "id": "fAHjRsLBV4O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [Load Dataset]\n",
        "\n",
        "# mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "\n",
        "# read file\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/Datasets/tennis.csv\"\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDHjtDeHp4FZ",
        "outputId": "de4e4829-818c-4993-fd80-ac0d8a0ccbb5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nULSfkqnNYXa",
        "outputId": "334eeaf8-faac-4881-eeab-4ced8a65247c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.values"
      ],
      "metadata": {
        "id": "deEkfIn6OlWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5846769b-89b5-47f8-ce4a-cfe9f84e34d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['sunny', 'hot', 'high', False, 'no'],\n",
              "       ['sunny', 'hot', 'high', True, 'no'],\n",
              "       ['overcast', 'hot', 'high', False, 'yes'],\n",
              "       ['rainy', 'mild', 'high', False, 'yes'],\n",
              "       ['rainy', 'cool', 'normal', False, 'yes'],\n",
              "       ['rainy', 'cool', 'normal', True, 'no'],\n",
              "       ['overcast', 'cool', 'normal', True, 'yes'],\n",
              "       ['sunny', 'mild', 'high', False, 'no'],\n",
              "       ['sunny', 'cool', 'normal', False, 'yes'],\n",
              "       ['rainy', 'mild', 'normal', False, 'yes'],\n",
              "       ['sunny', 'mild', 'normal', True, 'yes'],\n",
              "       ['overcast', 'mild', 'high', True, 'yes'],\n",
              "       ['overcast', 'hot', 'normal', False, 'yes'],\n",
              "       ['rainy', 'mild', 'high', True, 'no']], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJjKl2rtUAVk",
        "outputId": "e124092a-a90a-404c-fb68-17fd4671dcdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['outlook', 'temp', 'humidity', 'windy', 'play'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = df.keys()[-1]\n",
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MN1hkxZQU4_l",
        "outputId": "3f0370c5-832f-4d3e-d5a5-b17837b013a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'play'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check total number of labels."
      ],
      "metadata": {
        "id": "4vpNU_RgUVbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[target].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn8IMnOOUVYn",
        "outputId": "bbbdbc6d-04e8-4935-fb99-c344351c4337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['no', 'yes'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of 'no' and 'yes'\n",
        "print(df[target].value_counts()[df[target].unique()[0]])\n",
        "print(df[target].value_counts()[df[target].unique()[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqrt54R6UVWD",
        "outputId": "105de27f-694d-4654-e070-8315170e85f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Entropy of the whole dataset  \n",
        "Since the formula for **information gain** requires the entropy of the whole dataset, we compute that now:"
      ],
      "metadata": {
        "id": "e7kyok-bUVTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findEntropyWhole(df):\n",
        "  #last column in the dataframe is the target variable\n",
        "  target = df.keys()[-1]\n",
        "\n",
        "  #initailization\n",
        "  overall_entropy = 0\n",
        "\n",
        "  #possible values of target\n",
        "  values_in_target = df[target].unique()\n",
        "\n",
        "  for value in values_in_target:\n",
        "    p = df[target].value_counts()[value]/len(df[target])\n",
        "    overall_entropy += -p*np.log2(p)\n",
        "  return overall_entropy\n",
        "\n",
        "findEntropyWhole(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jwNPtpmUVNL",
        "outputId": "8ed75b9b-d499-42c2-92ec-88829b7834ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9402859586706309"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Calculating entropy of an attribute"
      ],
      "metadata": {
        "id": "-rYoImmgUU_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findEntropyAttribute(df, attribute):\n",
        "  #last column in the dataframe is the target variable\n",
        "  target = df.keys()[-1]\n",
        "\n",
        "  #initailization\n",
        "  overall_entropy = 0\n",
        "\n",
        "  #possible values of target\n",
        "  values_in_target = df[target].unique()\n",
        "\n",
        "  #this gives different features in that attribute like hot, cold in temperature\n",
        "  values_in_attribute = df[attribute].unique()\n",
        "\n",
        "  #initialize attribute entropy\n",
        "  entropy_attribute = 0\n",
        "\n",
        "  for value_in_attribute in values_in_attribute:\n",
        "    overall_entropy = 0\n",
        "    for value_in_target in values_in_target:\n",
        "      num = len(df[attribute][df[attribute] == value_in_attribute][df[target] == value_in_target])\n",
        "      den = len(df[attribute][df[attribute] == value_in_attribute])\n",
        "      p = num/(den+eps)\n",
        "      overall_entropy += -p*np.log2(p+eps)\n",
        "    p2 = den/len(df)\n",
        "    entropy_attribute += -p2*overall_entropy\n",
        "  return abs(entropy_attribute)"
      ],
      "metadata": {
        "id": "LMKbcl3bZVIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute entropy of different attributes now:"
      ],
      "metadata": {
        "id": "m5GB1LF5ZVGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for attribute in df.keys()[:-1]:\n",
        "  print('Entropy of attribute {}: {}'.format(attribute, findEntropyAttribute(df, attribute)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4sKgUTBZVD7",
        "outputId": "1ffddc30-f816-4c8d-ea35-9e619adf248a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of attribute outlook: 0.6935361388961914\n",
            "Entropy of attribute temp: 0.9110633930116756\n",
            "Entropy of attribute humidity: 0.7884504573082889\n",
            "Entropy of attribute windy: 0.892158928262361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the best attribute"
      ],
      "metadata": {
        "id": "dOQw6FNYZVBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_attribute_to_divide(df):\n",
        "  #Information gain\n",
        "  IG = []\n",
        "\n",
        "  # all column names\n",
        "  all_attributes_names = df.keys()[:-1]\n",
        "\n",
        "  for attribute in all_attributes_names:\n",
        "    # compute information gain for every attribute\n",
        "    IG.append(findEntropyWhole(df)- findEntropyAttribute(df, attribute))\n",
        "\n",
        "    # get index of attribute with best IG\n",
        "    max_IG_attribute_index = np.argmax(IG)\n",
        "\n",
        "    # best attribute name\n",
        "    best_attribute = all_attributes_names[max_IG_attribute_index]\n",
        "  return best_attribute\n",
        "\n",
        "find_best_attribute_to_divide(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r5pAE4aOZU_O",
        "outputId": "e333a861-04eb-425f-84eb-4605385257b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'outlook'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building the Decision tree"
      ],
      "metadata": {
        "id": "BZDHKG7WZU84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tree(df, tree=None):\n",
        "  # last column in dataframe\n",
        "  target = df.keys()[-1]\n",
        "\n",
        "  # Here we build our Decision tree\n",
        "\n",
        "  # get attribute with max IG\n",
        "  node = find_best_attribute_to_divide(df)\n",
        "\n",
        "  # get distinct values of that attribute\n",
        "  attributeValues = np.unique(df[node])\n",
        "\n",
        "  # create an empty dictionary to create tree\n",
        "  if tree is None:\n",
        "    tree = {}\n",
        "    tree[node] = {}\n",
        "\n",
        "  # we make loop to construct a tree by calling this function recursively\n",
        "  # in this we check if the subset is pure and stops if it is pure\n",
        "  for value in attributeValues:\n",
        "    subtable = df[df[node] == value].reset_index(drop=True)\n",
        "    clValue, counts = np.unique(subtable['play'], return_counts=True)\n",
        "    if len(counts) == 1: # Checking purity of subset\n",
        "      tree[node][value] = clValue[0]\n",
        "    else:\n",
        "      tree[node][value] = build_tree(subtable) # calling the function recursively\n",
        "  return tree\n",
        "\n",
        "build_tree(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w1MjDDxZU6U",
        "outputId": "53920c94-a6ba-45cc-e658-9b34670d2dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'outlook': {'overcast': 'yes',\n",
              "  'rainy': {'windy': {False: 'yes', True: 'no'}},\n",
              "  'sunny': {'humidity': {'high': 'no', 'normal': 'yes'}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ID3 in its pure form performs no backtracking in its search. Once it selects an attribute to test at a particular level in the tree, it never backtracks to reconsider this choice. Therefore, it is susceptible to the usual risk of hill-climbing search without backtracking: converging to local optimal solutions that may not be globally optimal."
      ],
      "metadata": {
        "id": "gm0g2TWxnlXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interview questions\n",
        "1. Entropy vs Gini Index vs IG\n",
        "2. When to use what?\n",
        "3. Decision tree in Regression"
      ],
      "metadata": {
        "id": "JFIPFfEGqHTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LuPZn-7eT97R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Decision_Trees.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}